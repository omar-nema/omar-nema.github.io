import{S as i,i as c,s as d,w as l,x as p,y as u,K as m,q as g,o as f,B as w,Q as h,T as z}from"../../chunks/vendor-a3e68c51.js";import{P as j}from"../../chunks/ProjectPage-2a0d2bd4.js";import{s as n}from"../../chunks/Header-0a2ddc6e.js";import"../../chunks/paths-28a87002.js";function v(t){let a,s;return a=new j({props:{imgs:t[0],desc:t[1],info:_,link:$}}),{c(){l(a.$$.fragment)},l(e){p(a.$$.fragment,e)},m(e,o){u(a,e,o),s=!0},p:m,i(e){s||(g(a.$$.fragment,e),s=!0)},o(e){f(a.$$.fragment,e),s=!1},d(e){w(a,e)}}}let _="2021 \u2022 Web App \u2022 Independent",$="https://omarnema.com/parsons-studio-1/qual/gaze-study-svelte/public/";function b(t,a,s){let e;return h(t,n,r=>s(2,e=r)),z(n,e="How We Gaze",e),[["/assets/gaze/gaze2.png","/assets/gaze/gaze1reaction.png","/assets/gaze/mainDemo.mp4"],["How We Gaze is a meta-gallery that shows how individuals gaze at pieces of artwork. The gazes hosted in the gallery are crowd-sourced - individuals are invited to view pieces of artwork, and see their gaze visualized in realtime.","How We Gaze was selected as a winner for the Pudding Cup 2021, an independent data visualization award.","This project was built using the Svelte.js as a framework, d3.js for visualization, webgazer.js for eye-tracking, and Firebase for data storage. Observable was used for data exploration."]]}class W extends i{constructor(a){super();c(this,a,b,v,d,{})}}export{W as default};
